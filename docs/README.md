# Using CrewAI to boost productivity in data analysis

This project uses CrewAI to create a team (or _crew_) of agents to:
* **Speed up and simplify** the process of data wrangling/analysis when presented with a new dataset
* Offer **strategies** to deal with any issues identified
* Develop preliminary **strategic insights** from the data
* Suggest areas for **further analysis**
* Use the results to create **business-ready presentations** using AI

The project uses CrewAI for a number of reasons, but mainly because it is open source
and serves as a flexible, application-agnostic agent-building framework.

A visual representation of the project is below:

![Project Diagram](https://github.com/user-attachments/assets/9cadd238-bb9d-4ca2-8280-f2090cf54692)


## Project Structure

- `data/raw`: Raw data files
- `docs/`: README and requirements.txt. Note that requirements.txt defines what libraries agents will use when executing code
- `notebooks/`: Jupyter notebooks for analysis
- `generated code/`: Code generated by the agents
- `presentations/`: AI-generated presentations synthesised from the agents' data analysis
- `reports/`: Analysis reports categorized by type

## The Data
Two datasets were used to develop the project, and both were created using mockaroo [https://www.mockaroo.com]:
1. transaction_data_with_errors.csv - 1,000 rows x 10 columns, with missing data
2. Superannuation.csv - 1,000 rows x 20 columns, with missing data

## Limitations
Using csv files with LLM API calls uses a lot of tokens. This project will exceed token limits for gpt-4o/4.1-mini when anything more than approx 700 lines x 20 columns
are read by an agent and sent to the API. As such, limiting how much of the file is read by each agent is necessary.

This also means that a comprehensive analysis of all the data isn't possible.
_The agents' output should serve as a baseline for further analysis and human-led insight generation._

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r docs/requirements.txt
```

NOTE: You will need to have Docker installed as CrewAI's CodeExecution tool runs code in a Docker environment.

## Usage

Run the main analysis notebook:
```bash
jupyter notebook notebooks/CrewAI_DA_Agents.ipynb
```
