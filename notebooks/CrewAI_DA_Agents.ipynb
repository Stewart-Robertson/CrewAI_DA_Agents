{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c0cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/docker\n"
     ]
    }
   ],
   "source": [
    "# Add /usr/local/bin to PATH so that docker is found (required for agent code execution)\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/bin\"\n",
    "\n",
    "import shutil\n",
    "print(shutil.which(\"docker\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9371b",
   "metadata": {},
   "source": [
    "# The dataset to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a396647",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"original_dataset\": \"Superannuation.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b544fc",
   "metadata": {},
   "source": [
    "# Imports, Environment Variables, Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456eaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Process, Crew\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import FileReadTool, FileWriterTool, DirectoryReadTool\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Specify the model. Comment out to use default\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4.1-mini'\n",
    "\n",
    "csv_reader = FileReadTool(file_path=inputs[\"original_dataset\"])\n",
    "file_writer = FileWriterTool()\n",
    "directory_reader = DirectoryReadTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ed380",
   "metadata": {},
   "source": [
    "## Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first agent will ingest the dataset, identify and report all quality issues, and provide strategies to resolve them\n",
    "data_cleaning_agent = Agent(\n",
    "    role=\"Senior Data Engineer\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Engineer specialising in data ingestion and cleaning. \n",
    "        Your primary goal is to ingest the raw data provided, identify all quality issues, and produce strategies for handling all identified quality issues.\n",
    "\n",
    "        Responsibilities:\n",
    "        1. Ingest the complete raw dataset\n",
    "        2. Identify and quantify all data quality issues\n",
    "        3. Develop strategies to resolve all data quality issues\n",
    "        4. Adhere to data engineering best practices\n",
    "        5. Produce a professional, high-quality, structured, and detailed markdown report on the quality issues and recommendations\n",
    "        \"\"\",\n",
    "    backstory=\"Senior Data Engineer experienced in Python, SQL, advanced data cleaning techniques, and producing top-tier data cleaning reports.\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=True,\n",
    "    tools=[csv_reader, file_writer, directory_reader],\n",
    "    #llm=claude_llm\n",
    ")\n",
    "\n",
    "# The second agent will perform exploratory data analysis on the dataset to derive insights and suggest further areas of analysis\n",
    "eda_agent = Agent(\n",
    "    role=\"Senior Data Scientist\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Scientist with deep expertise in exploratory data analysis and statistical analysis.\n",
    "        \n",
    "        Responsibilities\n",
    "        1. Ingest the complete raw dataset to perform analysis\n",
    "        2. Calculate statistics\n",
    "        3. Perform correlation analysis\n",
    "        4. Perform distribution analysis\n",
    "        5. Investigate any patterns that may be present in the data\n",
    "        6. Produce a professional, high-quality, structured, and detailed markdown report compiling the above findings\n",
    "        \"\"\",\n",
    "    backstory=\"A senior data scientist with extensive experience in exploratory data analysis and using python, pandas, and data manipulation, and with deep experience in creating top-tier EDA reports\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=1,\n",
    "    tools=[csv_reader, file_writer, directory_reader],\n",
    "    #llm=openai_llm\n",
    ")\n",
    "\n",
    "# The third agent will create a report on suggested data visualisations, and use python to visualise and save visualisations of the data\n",
    "data_visualisation_agent = Agent(\n",
    "    role=\"Senior Data Visualisation Engineer\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Visualisation Engineer with expertise in creating impactful visualisations that provide clear insight into data.\n",
    "        \n",
    "        Responsibilities:\n",
    "        1. Ingest the complete raw dataset\n",
    "        2. Create a visualisation plan describing the visualisations to be made\n",
    "        3. Save the visualisation plan in a detailed visualisation report in markdown format\n",
    "        4. Create the visualisations and save them as .png files following data visualisation best practices\n",
    "        \"\"\",\n",
    "    backstory=\"A senior data visualisation engineer with extensive experience in creating high-quality visualitsations and reports to be shared with stakeholders\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=1,\n",
    "    tools=[file_writer, directory_reader],\n",
    "    #llm=openai_llm\n",
    ")\n",
    "\n",
    "# The fourth and final agent will take the findings of preceding agents and compile all knowledge into a professional report on the dataset\n",
    "reporting_agent = Agent(\n",
    "    role=\"Senior Reporting Analyst\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Reporting Analyst with deep expertise in creating highly impactful business reports that are key in developing business strategies.\n",
    "        \n",
    "        Responsibilities:\n",
    "        1. Read all the markdown reports produced by the other agents\n",
    "        2. Compile all their findings into a comprehensive, detailed business report to be shared with key stakeholders and business leaders\n",
    "        3. Ensure all the important findings are included\n",
    "        4. Critically evaluate the dataset and provide possibilities for further analysis.\n",
    "        \"\"\",\n",
    "    backstory=\"A senior reporting analyst with extensive experience in creating in-depth, technical business data reports\",\n",
    "    allow_code_execution=False,\n",
    "    memory=True,\n",
    "    verbose=1,\n",
    "    tools=[file_writer, directory_reader],\n",
    "    #llm=openai_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf829689",
   "metadata": {},
   "source": [
    "## Creating Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7a07b",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The data cleaning tasks will be performed in sequence by the data_cleaning_agent \"\"\"\n",
    "\n",
    "data_cleaning_process = [\n",
    "    # Task 1: Initial Data Assessment\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Conduct an initial assessment of the {original_dataset}. Ingest the first 500 rows.\n",
    "            Specifically:\n",
    "            1. Identify all columns and their data types\n",
    "            2. Check for basic data quality issues\n",
    "            3. Be as thorough and detailed as possible\n",
    "            4. Compile all the findings into a report\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format in a folder called 'cleaning_reports/' containing:\n",
    "                - Column summary: a table of the columns and their data types\n",
    "                - Data quality metrics:\n",
    "                    - Number of rows\n",
    "                    - Number of columns\n",
    "                    - Number of missing values\n",
    "                    - Number of duplicates\n",
    "                    - Data types\n",
    "                    - A final section recommending next steps to be taken\n",
    "                \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"initial_data_assessment.md\",\n",
    "    ),\n",
    "\n",
    "    # Task 2: Missing Value Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and determine a strategy to handle missing values present, if there are any. Do not implement the strategy.\n",
    "            Specifically:\n",
    "            1. Ingest the file using pandas\n",
    "            2. Check for missing values in the dataset\n",
    "            3. Determine an appropriate handling strategy\n",
    "            4. Do not do any further data cleaning\n",
    "            5. Compile all the findings into a report\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format in a folder called 'cleaning_reports/' containing:\n",
    "                - Sections detailing the findings of the missing value analysis\n",
    "                - A strategy for handling missing values (if there are any)\n",
    "                - Tabulate findings where appropriate\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"missing_value_handling.md\",\n",
    "    ),\n",
    "    \n",
    "    # Task 3: Format Standardisation\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a strategy to standardise formats across the dataset, if formats are not standard. Do not implement the strategy.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file and perform data manipulation\n",
    "            2. Identify inconsistent formats\n",
    "            3. Develop a standardisation strategy (e.g., date formats, string formats, etc.)\n",
    "            4. Do not do any further data cleaning\n",
    "            5. Compile all the findings into a report\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format in a folder called 'cleaning_reports/' containing:\n",
    "                - All identified data format issues\n",
    "                - Tabulate findings where appropriate (e.g. a table with columns: column_name, data_format, required_format)\n",
    "                - A summary of the recommended strategy for handling format standardisation in the dataset\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"format_standardisation.md\",\n",
    "    ),\n",
    "    \n",
    "    # Task 4: Duplicate Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a strategy for handling duplicates in the dataset, if there are any. Do not implement the strategy.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file\n",
    "            2. Identify duplicates\n",
    "            3. Determine a strategy for handling duplicates.\n",
    "            4. Do not do any further data cleaning\n",
    "            5. Compile all the findings into a report\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format in a folder called 'cleaning_reports/' containing:\n",
    "                - All duplicates found (if there are any)\n",
    "                - Tabulate the columns and values of duplcates if ther are any\n",
    "                - A handling strategy for duplicates in the dataset if there are any\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"duplicate_handling.md\",\n",
    "    ),\n",
    "    \n",
    "    # Task 5: Outlier Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a strategy for handling outliers in the dataset, if there are any. Do not implement the strategy.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file\n",
    "            2. Identify outliers using statistical methods (e.g., Z-score, IQR)\n",
    "            3. Determine a strategy for handling outliers (e.g., removal, transformation, etc.)\n",
    "            4. Do not do any further data cleaning\n",
    "            5. Compile a report with the findings\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format in a folder called 'cleaning_reports/' containing:\n",
    "                - Comprehensive outlier analysis with statistical reasoning\n",
    "                - Strategies for handling any outliers identified\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"outlier_handling.md\",\n",
    "    ),\n",
    "\n",
    "    # Task 6: Final Summary\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Read all of the markdown files in the 'cleaning_reports/' directory (ending with .md) and compile a final data cleaning report.\n",
    "            Specifically:\n",
    "            1. Read the initial_data_assessment.md, missing_value_handling.md, data_type_standardisation.md, format_standardisation.md, duplicate_handling.md, and outlier_handling.md files\n",
    "            2. Compile these results into a beautifully formatted report\n",
    "            3. Ensure the report includes all business-critical information to be shared with senior management\n",
    "            \"\"\",\n",
    "        expected_output=\"\"\"\n",
    "            - A detailed, structured report in markdown format called 'final_cleaning_summary.md' containing:\n",
    "                - Initial data assessment results\n",
    "                - Columns with missing values and strategies for dealing with them\n",
    "                - Summary of data types and strategies for standardising them\n",
    "                - Columns with duplicates and the strategy for them\n",
    "                - All outliers identified and the strategy for dealing with them\n",
    "                - An overall summary of the cleaning process\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"final_cleaning_summary.md\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ecf6c",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The exploratory data analysis tasks will be performed in sequence by the eda_agent \"\"\"\n",
    "\n",
    "eda_process = [\n",
    "    # Task 1: Basic Statistics\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and create basic statistics for the dataset.\n",
    "            Specifically:\n",
    "            1. Calculate descriptive statistics for numerical columns (where it makes sense)\n",
    "            2. Calculate descriptive statistics for categorical columns (where it makes sense) \n",
    "            3. Be as thorough and detailed as possible\n",
    "            4. Infer any important insights from the statistics\n",
    "            5. Compile a report with the findings\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called eda_statistics.md in markdown format with summary statistics and any insights in a directory called eda_reports/\n",
    "            - The report should contain at least:\n",
    "                - Summary statistics for numerical columns (tabulate where necessary)\n",
    "                - Summary statistics for categorical columns (tabulate where necessary)\n",
    "                - Any important insights that can be inferred\n",
    "                - Recommendations for future statistical analysis\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_statistics.md\",\n",
    "        #output_pydantic=BasicStatistics\n",
    "    ),\n",
    "    \n",
    "    # Task 2: Correlation Analysis\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and analyse relationships between variables.\n",
    "            Specifically:\n",
    "            1. Calculate correlations\n",
    "            2. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            3. Infer any important insights from the correlations\n",
    "            4. Compile a report with the findings\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called eda_correlation.md in markdown format with summary statistics and any insights in a directory called eda_reports/\n",
    "            - The report should contain at least:\n",
    "                - A tabulated correlation matrix\n",
    "                - Any insights generated from the correlation analysis\n",
    "                - Recommendations for further correlation analysis\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_correlation.md\",\n",
    "        #output_pydantic=CorrelationAnalysis\n",
    "    ),\n",
    "    \n",
    "    # Task 3: Distribution Analysis\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and analyse variable distributions.\n",
    "            Specifically:\n",
    "            1. Calculate distributions\n",
    "            2. Identify outliers\n",
    "            3. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            4. Infer any important insights from the distributions\n",
    "            5. Compile a report with the findings\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called eda_distribution.md in markdown format with summary statistics and any insights in a directory called eda_reports/\n",
    "            - The report should contain at least:\n",
    "                - Table(s) summarising the distributions in the dataset\n",
    "                - Summary statistics for the distributions\n",
    "                - Recommendations for further distribution analysis\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_distribution.md\",\n",
    "        #output_pydantic=DistributionAnalysis\n",
    "    ),\n",
    "    \n",
    "    # Task 4: Pattern Identification\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and identify patterns and trends.\n",
    "            Specifically:\n",
    "            1. Look for temporal patterns\n",
    "            2. Identify clusters\n",
    "            3. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            4. Infer any important insights from the patterns\n",
    "            5. Compile a report with the findings\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called eda_patterns.md in markdown format with summary statistics and any insights in a directory called eda_reports/\n",
    "            - The report should contain at least:\n",
    "                - An analysis of temporal patterns\n",
    "                - An analysis of clusters\n",
    "                - Recommendations for further pattern analysis\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_patterns.md\",\n",
    "        #output_pydantic=PatternIdentification\n",
    "    ),\n",
    "    \n",
    "    # Task 5: Final Summary\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Create comprehensive EDA report to be shared with senior business stakeholders,\n",
    "            Specifically:\n",
    "            1. Compile all findings from previous tasks. This will require reading the previous markdown files inside the 'eda_reports/' directory:\n",
    "                - eda_statistics.md\n",
    "                - eda_correlation.md\n",
    "                - eda_distribution.md\n",
    "                - eda_patterns.md\n",
    "            2. Use a professional, highly analytical tone\n",
    "            3. Ensure the report is structured well, with clear sections, insights, and recommendations\n",
    "            \"\"\",\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called eda_summary.md in markdown format in the eda_reports/ directory with all findings and insights.\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_summary.md\",\n",
    "        #output_pydantic=FinalSummary\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176a96d",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The data visualisation tasks will be completed in sequence by the data_visualisation_agent \"\"\"\n",
    "\n",
    "# Visualisation Tasks\n",
    "visualisation_process = [\n",
    "    # Task 1: Visualisation Planning\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Read eda_analysis.md and plan visualisations to be created from the {original_dataset}. Do not create any visualisations yet.\n",
    "            Specifically:\n",
    "            1. Read the eda_summary.md file to understand the findings\n",
    "            2. Ingest the first 500 rows of {original_dataset} file using pandas\n",
    "            3. Examine columns to determine suitable visualisation types\n",
    "            4. For categorical columns, choose appropriate visualisation types such as bar charts, pie charts, etc.\n",
    "            5. For numerical columns, choose appropriate visualisation types such as histograms, scatter plots, etc.\n",
    "            6. Justify the choice of visualisation types\n",
    "            7. Compile your analysis into a report\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A rich, detailed report called visualisation_planning.md in markdown format in a directory called visualisation_reports/.\n",
    "            - The report should include at least:\n",
    "                - A table containing columns and recommended visualisation types\n",
    "                - Justification for the choice of visualisations\n",
    "                - Recommendations for further visual analysis\n",
    "            \"\"\",\n",
    "        agent=data_visualisation_agent,\n",
    "        output_file=\"visualisation_planning.md\",\n",
    "        #output_pydantic=VisualisationPlanning\n",
    "    ),\n",
    "    \n",
    "    # Task 2: Create Simple Charts\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Read visualisation_planning.json and the {original_dataset} files and use them to create visualisations.\n",
    "            Specifically:\n",
    "            1. Ingest the first 500 rows of {original_dataset} file using pandas\n",
    "            2. Use matplotlib or seaborn for visualisation\n",
    "            3. Create charts using the proposed visualisation types\n",
    "            4. Include legends and titles\n",
    "            5. Produce a python file that will save various visualisations when run\n",
    "            \"\"\".format(original_dataset=inputs[\"original dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - Numerous saved .png files in the visualisation_reports/ directory\n",
    "            \"\"\",\n",
    "        agent=data_visualisation_agent,\n",
    "        #output_files=[\"visualisation_summary.md\"],\n",
    "        #output_pydantic=VisualisationCreation\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845f371",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final report will be compiled by the reporting_agent\n",
    "\n",
    "reporting_process = [\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Read the data_cleaning_summary.md, eda_summary.md, and visualisation_summary.md reports and create a final report.\n",
    "            - data_cleaning_summary.md is located in the cleaning_reports/ directory\n",
    "            - eda_summary.md is located in the eda_reports/ directory\n",
    "            - visualisation_summary.md is located in the visualisation_reports/ directory\n",
    "            Specifically:\n",
    "            1. Compile all findings from previous reports\n",
    "            2. Include links to visualisations using the paths to the visualisation files in visualisation_summary.md\n",
    "            3. Critically evaluate the findings and provide actionable recommendations and possibilities for further analysis or process improvements\n",
    "            3. Save the report as final_report.md\n",
    "            \"\"\",\n",
    "        expected_output=\"\"\"\n",
    "            A rich, detailed report in markdown format with all findings and insights inside a directory called 'final_report/'.\n",
    "            - The report should include:\n",
    "                - Data cleaning summary\n",
    "                - EDA analysis summary\n",
    "                - Visualisation summary\n",
    "                - Critical evaluation of findings\n",
    "                - Actionable recommendations\n",
    "                - Possibilities for further analysis or process improvements\n",
    "            \"\"\",\n",
    "        agent=reporting_agent,\n",
    "        output_file=\"final_report.md\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2a3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the tasks into a single process\n",
    "all_tasks = data_cleaning_process + eda_process + visualisation_process + reporting_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60358a9",
   "metadata": {},
   "source": [
    "## Create the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "data_analysis_crew = Crew(\n",
    "    agents=[\n",
    "        data_cleaning_agent,\n",
    "        eda_agent,\n",
    "        data_visualisation_agent,\n",
    "        reporting_agent,\n",
    "    ],\n",
    "\n",
    "    tasks=all_tasks,\n",
    "    manager_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2), # try to make the manager more deterministic\n",
    "    process=Process.hierarchical,\n",
    "    verbose=False,\n",
    "    memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb951a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to give feedback on task completions\n",
    "#data_analysis_crew.train(n_iterations=1, filename=\"crew_training.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bbe9d",
   "metadata": {},
   "source": [
    "## Kickoff the crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_analysis_crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "filepath = \"final_report.md\"\n",
    "display(Markdown(filename=filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8910a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
